import json
import asyncio
import logging
from datetime import datetime
from typing import List, Dict, Any, Optional

from app.services.llm import llm_service
from app.services.dsml_parser import DSMLParser
from app.database import (
    create_coach_session, get_total_coach_messages, increment_coach_message_count,
    remember_fact, recall_fact, get_all_memories,
    update_mastery, get_mastery,
    get_session_vocab, get_story
)

logger = logging.getLogger(__name__)

# Tool Definitions
COACH_TOOLS = [
    {
        "type": "function", # DeepSeek/OpenAI format
        "function": {
            "name": "show_vocabulary",
            "description": "Display vocabulary words to the user for teaching or review.",
            "parameters": {
                "type": "object",
                "properties": {
                    "words": {
                        "type": "array",
                        "items": {"type": "string"},
                        "description": "List of words to show"
                    },
                    "mode": {
                        "type": "string",
                        "enum": ["cards", "grid", "list"],
                        "description": "Visual presentation mode. 'cards' for teaching new words, 'grid' for review."
                    },
                    "topic": {"type": "string", "description": "The theme/topic these words belong to"}
                },
                "required": ["words", "mode"]
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "present_story",
            "description": "Show a story to the user, optionally highlighting specific words.",
            "parameters": {
                "type": "object",
                "properties": {
                    "topic": {"type": "string", "description": "Story topic"},
                    "tense": {"type": "string", "description": "Target grammar tense"},
                    "highlights": {
                        "type": "array", 
                        "items": {"type": "string"},
                        "description": "Words to highlight in the story"
                    }
                },
                "required": ["topic", "tense"]
            }
        }
    },
    {
        "type": "function",
        "function": {
           "name": "remember_user_fact",
           "description": "Save important information about the user (interests, goals, struggles) to long-term memory.",
           "parameters": {
                "type": "object",
                "properties": {
                    "category": {"type": "string", "description": "Category: interests, struggles, goals, style"},
                    "fact": {"type": "string", "description": "The content to remember"}
                },
                "required": ["category", "fact"]
           }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "update_student_progress",
            "description": "Update the student's mastery level for a specific topic or skill.",
            "parameters": {
                "type": "object",
                "properties": {
                    "topic": {"type": "string", "description": "The skill or topic (e.g. 'past_simple', 'cooking_vocab')"},
                    "level": {"type": "integer", "description": "Mastery level 0-5 (5 is expert)"},
                    "observation": {"type": "string", "description": "Notes about their performance"}
                },
                "required": ["topic", "level"]
            }
        }
    }
]

class CoachService:
    def __init__(self):
        # In-memory session store (Active context)
        # In production with multiple workers, this should be in Redis
        # For now, we rely on in-memory + DB persistence for history
        self.active_sessions: Dict[str, Dict[str, Any]] = {}

    async def start_session(self, user_id: str = "default_user") -> Dict[str, Any]:
        """Initialize a new coaching session."""
        
        # 1. Retrieve User Context
        memories = await get_all_memories(user_id)
        
        # 2. Construct System Prompt
        system_prompt = self._build_system_prompt(memories)
        
        # 3. Create DB Session
        session_id = await create_coach_session(user_id)
        
        # 4. Init In-Memory Context
        self.active_sessions[session_id] = {
            "user_id": user_id,
            "messages": [{"role": "system", "content": system_prompt}],
            "created_at": datetime.now()
        }
        
        # 5. Initial Greeting (Generated by LLM)
        # We can trigger the first turn properly
        initial_response = await self.handle_turn(session_id, "Hello Coach!")
        
        return {
            "session_id": session_id,
            "message": initial_response["message"],
            "tool_calls": initial_response.get("tool_calls", [])
        }

    async def handle_turn(self, session_id: str, user_input: str) -> Dict[str, Any]:
        """Process one conversational turn."""
        if session_id not in self.active_sessions:
            # Try to restore? For now error
            return {"error": "Session not active"}
            
        context = self.active_sessions[session_id]
        context["messages"].append({"role": "user", "content": user_input})
        
        # API Call to DeepSeek
        try:
            # We use the raw client for full tool control
            client = llm_service.async_client
            if not client:
                 return {"message": "Service unavailable (LLM not configured)."}

            response = await client.chat.completions.create(
                model=llm_service.model_name or "deepseek-chat",
                messages=context["messages"],
                tools=COACH_TOOLS,
                tool_choice="auto",
                temperature=0.7
            )
            
            response_msg = response.choices[0].message
            tool_calls = response_msg.tool_calls
            
            # Add assistant message to history
            context["messages"].append(response_msg)
            
            final_content = response_msg.content or ""
            executed_actions = []

            # Handle Tools
            # Handle Tools (API or DSML)
            dsml_tools = []
            if not tool_calls and final_content:
                # Check for Raw DSML in content
                parsed_tools = DSMLParser.parse(final_content)
                if parsed_tools:
                    dsml_tools = parsed_tools
                    # Strip the raw XML from the message the user sees
                    final_content = DSMLParser.strip_dsml(final_content)
            
            # Combine API tools and DSML tools
            all_tool_calls = []
            
            if tool_calls:
                 for tc in tool_calls:
                     all_tool_calls.append({
                         "id": tc.id,
                         "name": tc.function.name,
                         "args": json.loads(tc.function.arguments)
                     })
            
            if dsml_tools:
                for dt in dsml_tools:
                    all_tool_calls.append({
                        "id": "dsml_call", # Synthetic ID
                        "name": dt["name"],
                        "args": dt["arguments"]
                    })

            if all_tool_calls:
                for tool_call in all_tool_calls:
                    func_name = tool_call["name"]
                    args = tool_call["args"]
                    
                    # Execute locally
                    result = await self._execute_tool(context["user_id"], func_name, args)
                    
                    # Append result to history
                    context["messages"].append({
                        "role": "tool",
                        "tool_call_id": tool_call["id"],
                        "content": json.dumps(result)
                    })
                    
                    executed_actions.append({
                        "tool": func_name,
                        "args": args,
                        "result": result
                    })

                # If tools were called, we might want a follow-up response from the model
                # referencing the tool output.
                # For simplicity in V1, we just return the tool calls to Frontend 
                # and let Frontend display them. 
                # BUT, if the model didn't say anything (content is None), we might need a follow up.
                if not final_content:
                     followup = await client.chat.completions.create(
                        model=llm_service.model_name,
                        messages=context["messages"]
                    )
                     final_content = followup.choices[0].message.content
                     context["messages"].append(followup.choices[0].message)

            # Persist count
            await increment_coach_message_count(session_id)
            
            return {
                "message": final_content,
                "tool_calls": executed_actions
            }

        except Exception as e:
            logger.exception("Coach Error")
            return {"message": "I'm having trouble connecting right now."}

    async def _execute_tool(self, user_id: str, name: str, args: Dict[str, Any]) -> Any:
        """Route and execute tools."""
        from app.services.aui import aui_renderer
        
        # Default level for now, could fetch from DB
        # user_level = await get_mastery(user_id, "general") or 1
        user_level = 1 
        
        if name == "remember_user_fact":
            await remember_fact(user_id, args["category"], {"val": args["fact"]})
            return {"status": "saved"}
            
        elif name == "update_student_progress":
            await update_mastery(user_id, args["topic"], args["level"], args.get("observation"))
            return {"status": "updated"}
            
        elif name == "show_vocabulary":
            # Delegate to AUI Renderer
            packet = aui_renderer.render("show_vocabulary", args, user_level)
            return packet.model_dump()

        elif name == "present_story":
            # Attempt to fetch story content if it exists
            story = await get_story(args["topic"], args["tense"])
            if not story:
                # Fallback: Generate story on the fly
                # print(f"Story not found for {args['topic']}, generating...")
                story = await self._generate_story(args["topic"], args["tense"])

            if story:
                # Delegate to AUI Renderer
                # Merge args with story content
                render_data = {**args, **story}
                packet = aui_renderer.render("present_story", render_data, user_level)
                return packet.model_dump()
            
            return {
                "status": "error", 
                "info": "Failed to generate story content.",
                "content": "I couldn't generate a story for that topic right now."
            }
            
        return {"status": "unknown_tool"}
        
    def _build_system_prompt(self, memories: Dict[str, Any]) -> str:
        """Construct a personalized system prompt."""
        base = """You are the 'Neural Link Coach', an intelligent English tutor.
Your goal is to guide the user from Passive Understanding to Active Usage.
        
Style:
- Encouraging but precise.
- Use the 'Cyber-Noir' aesthetic in your speech (mental gym, neural pathways, calibration).
- Be concise. Don't lecture.
        
Methodology:
1. Teach vocabulary in context.
2. Verify understanding with drills.
3. Push for active usage in roleplay.
        
KNOWN USER DATA:
"""
        for k, v in memories.items():
            base += f"- {k}: {v}\n"
            
        base += "\nUse your tools to show content. When you teach words, call 'show_vocabulary'. When you want to practice in context, call 'present_story'."
        return base

    async def _generate_story(self, topic: str, tense: str) -> Optional[Dict[str, Any]]:
        """Generate a short story using the LLM when not found in DB."""
        try:
            client = llm_service.async_client
            if not client: return None

            prompt = f"""Write a short English story (approx 150 words) about '{topic}'.
Target Grammar: {tense}.
Format: Return ONLY the story text. Title first on its own line, then the body."""

            response = await client.chat.completions.create(
                model=llm_service.model_name or "deepseek-chat",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.7
            )
            content = response.choices[0].message.content
            
            # Simple parsing of Title / Body
            lines = content.strip().split('\n')
            title = lines[0].replace("#", "").strip()
            body = "\n".join(lines[1:]).strip()

            return {
                "title": title,
                "content": body,
                "highlights": [], # Could implement keyword extraction if needed
                "grammar_notes": []
            }
        except Exception as e:
            logger.exception("Story Gen Error")
            return None

# Singleton
coach_service = CoachService()
